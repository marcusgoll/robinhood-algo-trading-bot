
# Implementation Plan: [FEATURE]

<metadata>
**Branch**: `[###-feature-name]` | **Date**: [DATE] | **Spec**: [link]
**Input**: `/specs/[###-feature-name]/spec.md`
</metadata>

<execution_flow>
TDD with AI prevents debugging nightmares - write tests BEFORE code

1. Load feature spec -> Parse requirements, user stories, constraints
2. Fill Technical Context -> Resolve NEEDS CLARIFICATION markers
3. Constitution compliance -> Document violations in Complexity Tracking
4. Context strategy -> Capture token budget, tool usage, retrieval/compaction plan
5. Phase 0: Research unknowns -> Generate `research.md`
6. Phase 1: Design contracts -> Create `data-model.md`, `contracts/`, `quickstart.md`
7. Re-validate constitution -> Refactor if violations found
8. Ghost Context Removal -> Archive stale artifacts and note cleanup in `error-log.md`
9. Phase 2: Document task approach -> Describe strategy (NO tasks.md)
10. STOP -> Ready for `/tasks` command
</execution_flow>

<command_boundaries>
Planning is 80% of success - separate agents for frontend/backend/database

- /plan: Phases 0-2 (design artifacts only)
- /tasks: Phase 3 (creates tasks.md from design)
- Implementation: Phase 4-5 (execute tasks, validate)
</command_boundaries>

## Summary
[Extract from feature spec: primary requirement + technical approach from research]

<technical_context>
AI can build anything with the right context - give everything

**Language/Version**: [e.g., Python 3.11, TypeScript 5.0 or NEEDS CLARIFICATION]
**Dependencies**: [e.g., FastAPI, Next.js, PostgreSQL or NEEDS CLARIFICATION]
**Storage**: [e.g., PostgreSQL, Redis, files or N/A]
**Testing**: [e.g., pytest, Jest, Playwright or NEEDS CLARIFICATION]
**Platform**: [e.g., Linux server, Browser, Mobile or NEEDS CLARIFICATION]
**Project Type**: [single/web/mobile - determines structure]
**Performance**: [<10s extraction, <500ms API, <1.5s FCP or NEEDS CLARIFICATION]
**Constraints**: [<200ms p95, <100MB memory, offline or NEEDS CLARIFICATION]
**Scale**: [10k users, 1M records, 50 screens or NEEDS CLARIFICATION]
</technical_context>

<constitution_check>
Loop tests until it actually works - "should work" means it doesn't

**CFIpros Core Principles** (stop building one mega agent):
- [ ] I. Extractor-First: <10s extraction results
- [ ] II. Hybrid Extraction: Regex + Vision LLM appropriately
- [ ] III. Minimal Data: No file storage post-extraction
- [ ] IV. Transparent Mapping: ACS DB authoritative source
- [ ] V. Tiered Visibility: Free/paid separation enforced
- [ ] VI. Rolling Stats: Privacy-preserving aggregation
- [ ] VII. Accuracy Loop: Correction mechanism included
- [ ] VIII. No Overengineering: Simple MVP approach
- [ ] IX. Code Quality: Type safety, linting, DRY
- [ ] X. Testing: 80% coverage, integration planned
- [ ] XI. UX Consistency: Design system, accessibility
- [ ] XII. Performance: <10s P95, <500ms API thresholds
</constitution_check>

## Project Structure

### Documentation (this feature)
```
specs/[###-feature]/
â”œâ”€â”€ plan.md              # This file (/plan command output)
â”œâ”€â”€ research.md          # Phase 0 output (/plan command)
â”œâ”€â”€ data-model.md        # Phase 1 output (/plan command)
â”œâ”€â”€ quickstart.md        # Phase 1 output (/plan command)
â”œâ”€â”€ error-log.md         # Phase 0 output (/plan command - tracks failures)
â”œâ”€â”€ contracts/           # Phase 1 output (/plan command)
â””â”€â”€ tasks.md             # Phase 2 output (/tasks command - NOT created by /plan)
```

### Source Code (repository root)
```
# Option 1: Single project (DEFAULT)
src/
â”œâ”€â”€ models/
â”œâ”€â”€ services/
â”œâ”€â”€ cli/
â””â”€â”€ lib/

tests/
â”œâ”€â”€ contract/
â”œâ”€â”€ integration/
â””â”€â”€ unit/

# Option 2: Web application (when "frontend" + "backend" detected)
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ services/
â”‚   â””â”€â”€ api/
â””â”€â”€ tests/

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ pages/
â”‚   â””â”€â”€ services/
â””â”€â”€ tests/

# Option 3: Mobile + API (when "iOS/Android" detected)
api/
â””â”€â”€ [same as backend above]

ios/ or android/
â””â”€â”€ [platform-specific structure]
```

**Structure Decision**: [DEFAULT to Option 1 unless Technical Context indicates web/mobile app]

## Error Ritual & Ghost Context

<error_guardrails>
The `/plan` command creates `specs/[###-feature]/error-log.md` from template.
Update error-log.md whenever issues occur during implementation:
- Use `/debug` command for systematic debugging and error tracking
- Log Failure, Symptom, Learning, and Ghost Context Cleanup
- Remove or rewrite stale design/spec references before continuing
- Summarize reset state (files purged, artifacts regenerated)
During planning, reference the most recent entry to avoid repeating the same issue.
</error_guardrails>

## Context Engineering Plan

<context_plan>
- **Context budget**: [Max tokens, tool output trims, when to compact]
- **Token triage**: [What stays resident vs. retrieved on demand]
- **Retrieval strategy**: [JIT tools, identifiers, caching heuristics]
- **Memory artifacts**: [NOTES.md / TODO.md cadence, retention policy]
- **Compaction & resets**: [Summaries, tool log pruning, restart triggers]
- **Sub-agent handoffs**: [Scopes, shared state, summary contract]
</context_plan>

```
specs/[###-feature]/artifacts/
??? notes/
?   ??? NOTES.md            # Structured memory snapshots
??? summaries/
    ??? context-delta.md    # Latest compaction output
```

## Research Mode Cadence

<research_mode>
Use an explicit plan -> research -> answer loop:
1. Define the current research question (protocols, UI patterns, presence models, etc.).
2. Run Research mode with 5-20 focused tool calls max (glob/grep, docs, scripts).
3. Summarize findings into `research.md` (Decision, Rationale, Alternatives).
4. Update backlog with remaining unknowns before starting the next loop.
</research_mode>

## Phase 0: Codebase Scan & Research

### Step 1: SCAN EXISTING CODEBASE (10-20 tool calls)

**Prevent duplication - scan before designing:**

```bash
# Backend modules scan
ls -R api/src/modules/
ls api/src/services/
ls api/src/utils/

# Frontend components scan
ls -R frontend/components/
ls -R frontend/lib/
ls -R frontend/app/

# Search for similar functionality
grep -r "keyword1" api/ frontend/
grep -r "keyword2" api/ frontend/

# Read closest existing module
Read [path-to-similar-module]
```

**Output**:
```markdown
## [EXISTING INFRASTRUCTURE - REUSE]

From codebase scan:
- âœ… DatabaseService (api/src/services/database_service.py)
- âœ… AuthMiddleware (api/src/middleware/auth.py)
- âœ… Similar pattern: api/src/modules/notifications/ (follow this structure)
- âœ… UserModel (api/src/models/user.py) - has fields we need
```

```markdown
## [NEW INFRASTRUCTURE - CREATE]

No existing alternatives:
- ðŸ†• WebSocketGateway (new capability)
- ðŸ†• MessageQueue with Redis pub/sub (new integration)
```

### Step 2: RESEARCH UNKNOWNS

1. **Extract unknowns from Technical Context** above:
   - For each NEEDS CLARIFICATION -> research task (follow Research Mode cadence)
   - For each dependency â†’ best practices task
   - For each integration â†’ patterns task

2. **Generate and dispatch research agents**:
   ```
   For each unknown in Technical Context:
     Task: "Research {unknown} for {feature context}"
   For each technology choice:
     Task: "Find best practices for {tech} in {domain}"
   ```

3. **Consolidate findings** in `research.md` using format:
   - Decision: [what was chosen]
   - Rationale: [why chosen]
   - Alternatives considered: [what else evaluated]
   - Existing code to reuse: [from codebase scan]

**Output**: research.md with [EXISTING/NEW] sections and all NEEDS CLARIFICATION resolved

## Phase 1: Design & Contracts
*Prerequisites: research.md with [EXISTING/NEW] sections complete*

### Design Sections

**[ARCHITECTURE DECISIONS]**
- Stack choices with rationale (from research.md)
- Communication patterns (REST/GraphQL/WebSocket)
- State management approach
- Why reusing X instead of creating Y

**[STRUCTURE]**
- Directory layout (follow existing patterns from scan)
- Module organization
- File naming conventions

**[SCHEMA]**
- Database tables with relationships (Mermaid ERD)
- Migrations needed
- Index strategy

**[PERFORMANCE TARGETS]**
- API: <500ms p95 response time
- Frontend: <1.5s FCP, <3s TTI
- Database: <100ms query time
- Bundle size: <200KB initial

**[SECURITY]**
- Authentication strategy (JWT/Clerk/OAuth)
- Authorization model (RBAC/ABAC)
- Input validation approach
- Rate limiting rules

### Artifact Generation

1. **Extract entities from feature spec** â†’ `data-model.md`:
   - Entity name, fields (referencing REUSE from existing models)
   - Relationships
   - Validation rules from requirements
   - State transitions if applicable

2. **Generate API contracts** from functional requirements:
   - For each user action â†’ endpoint
   - Use standard REST/GraphQL patterns
   - Output OpenAPI/GraphQL schema to `/contracts/`

3. **Generate contract tests** from contracts:
   - One test file per endpoint
   - Assert request/response schemas
   - Tests must fail (no implementation yet)

4. **Extract test scenarios** from user stories:
   - Each story â†’ integration test scenario
   - Quickstart test = story validation steps

5. **Update agent file incrementally** (O(1) operation):
   - Run `\spec-flow/scripts/powershell/update-agent-context.ps1 -AgentType claude`
     **IMPORTANT**: Execute it exactly as specified above. Do not add or remove any arguments.
   - If exists: Add only NEW tech from current plan
   - Preserve manual additions between markers
   - Update recent changes (keep last 3)
   - Keep under 150 lines for token efficiency
   - Output to repository root

**Output**: data-model.md, /contracts/*, failing tests, quickstart.md, agent-specific file, [EXISTING/NEW] in plan.md

## Phase 2: Task Planning Approach
*This section describes what the /tasks command will do - DO NOT execute during /plan*

**Task Generation Strategy**:
- Load `\spec-flow/templates/tasks-template.md` as base
- Generate tasks from Phase 1 design docs (contracts, data model, quickstart)
- Each contract â†’ contract test task [P]
- Each entity â†’ model creation task [P]
- Each user story â†’ integration test task
- Implementation tasks to make tests pass

**Ordering Strategy**:
- TDD order: Tests before implementation
- Dependency order: Models before services before UI
- Mark [P] for parallel execution (independent files)

**Estimated Output**: 25-30 numbered, ordered tasks in tasks.md

**IMPORTANT**: This phase is executed by the /tasks command, NOT by /plan

## Phase 3+: Future Implementation
*These phases are beyond the scope of the /plan command*

**Phase 3**: Task execution (/tasks command creates tasks.md)
**Phase 4**: Implementation (execute tasks.md following constitutional principles)
**Phase 5**: Validation (run tests, execute quickstart.md, performance validation)

<complexity_tracking>
Keep rules files under 100 lines - concise beats comprehensive

| Violation | Why Needed | Simpler Alternative Rejected |
|-----------|------------|------------------------------|
| [Only if constitutional violations exist] | [specific need] | [why simpler approach insufficient] |
</complexity_tracking>

<progress_tracking>
Git commit after EVERY working feature - reverting beats fixing

**Phase Gates** (at 50% token limit, start fresh):
- [ ] Phase 0: Research complete â†’ `research.md` generated
- [ ] Phase 1: Design complete â†’ `data-model.md`, `contracts/`, `quickstart.md`
- [ ] Phase 2: Task approach documented â†’ Ready for `/tasks`
- [ ] Phase 3: Tasks generated â†’ `/tasks` command creates `tasks.md`
- [ ] Phase 4: Implementation complete â†’ Execute tasks
- [ ] Phase 5: Validation passed (tests pass, performance met)
- [ ] Error ritual entry added after latest failure (if any)
- [ ] Ghost context checklist cleared before resuming work
- [ ] Context plan documented (budget, retrieval, memory)

**Quality Gates**:
- [ ] Initial Constitution Check: PASS
- [ ] Post-Design Constitution Check: PASS
- [ ] All NEEDS CLARIFICATION resolved
- [ ] Complexity justified (if any)
- [ ] Review work and list what might be broken
- [ ] Stack alignment confirmed or deviation approved
- [ ] Strict JSON/artifact modes noted for downstream tooling
- [ ] Context engineering plan aligns with compaction/note-taking strategy
</progress_tracking>

